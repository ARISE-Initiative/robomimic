{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2b15f2e",
   "metadata": {},
   "source": [
    "# Run a trained policy\n",
    "\n",
    "This notebook will provide examples on how to run a trained policy and visualize the rollout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "000a4ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import h5py\n",
    "import imageio\n",
    "import numpy as np\n",
    "import os\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "\n",
    "import robomimic\n",
    "import robomimic.utils.file_utils as FileUtils\n",
    "import robomimic.utils.torch_utils as TorchUtils\n",
    "import robomimic.utils.tensor_utils as TensorUtils\n",
    "import robomimic.utils.obs_utils as ObsUtils\n",
    "from robomimic.envs.env_base import EnvBase\n",
    "from robomimic.algo import RolloutPolicy\n",
    "\n",
    "import urllib.request\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47427159",
   "metadata": {},
   "source": [
    "### Download policy checkpoint\n",
    "First, let's try downloading a pretrained model from our model zoo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9dfdfe5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get pretrained checkpooint from the model zoo\n",
    "\n",
    "ckpt_path = \"lift_ph_low_dim_epoch_1000_succ_100.pth\"\n",
    "# Lift (Proficient Human)\n",
    "urllib.request.urlretrieve(\n",
    "    \"http://downloads.cs.stanford.edu/downloads/rt_benchmark/model_zoo/lift/bc_rnn/lift_ph_low_dim_epoch_1000_succ_100.pth\",\n",
    "    filename=ckpt_path\n",
    ")\n",
    "\n",
    "assert os.path.exists(ckpt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2c25c6",
   "metadata": {},
   "source": [
    "### Loading trained policy\n",
    "We have a convenient function called `policy_from_checkpoint` that takes care of building the correct model from the checkpoint and load the trained weights. Of course you could also load the checkpoint manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf84aed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Loaded Config =============\n",
      "{\n",
      "    \"algo_name\": \"bc\",\n",
      "    \"experiment\": {\n",
      "        \"name\": \"core_bc_rnn_lift_ph_low_dim\",\n",
      "        \"validate\": true,\n",
      "        \"logging\": {\n",
      "            \"terminal_output_to_txt\": true,\n",
      "            \"log_tb\": true\n",
      "        },\n",
      "        \"save\": {\n",
      "            \"enabled\": true,\n",
      "            \"every_n_seconds\": null,\n",
      "            \"every_n_epochs\": 50,\n",
      "            \"epochs\": [],\n",
      "            \"on_best_validation\": false,\n",
      "            \"on_best_rollout_return\": false,\n",
      "            \"on_best_rollout_success_rate\": true\n",
      "        },\n",
      "        \"epoch_every_n_steps\": 100,\n",
      "        \"validation_epoch_every_n_steps\": 10,\n",
      "        \"env\": null,\n",
      "        \"additional_envs\": null,\n",
      "        \"render\": false,\n",
      "        \"render_video\": true,\n",
      "        \"keep_all_videos\": false,\n",
      "        \"video_skip\": 5,\n",
      "        \"rollout\": {\n",
      "            \"enabled\": true,\n",
      "            \"n\": 50,\n",
      "            \"horizon\": 400,\n",
      "            \"rate\": 50,\n",
      "            \"warmstart\": 0,\n",
      "            \"terminate_on_success\": true\n",
      "        }\n",
      "    },\n",
      "    \"train\": {\n",
      "        \"data\": \"/cvgl2/u/amandlek/batch_datasets/final_benchmark_datasets/lift/ph/low_dim.hdf5\",\n",
      "        \"output_dir\": \"/cvgl2/u/amandlek/batch_datasets/verification_run_results/core/bc_rnn/lift/ph/low_dim/trained_models\",\n",
      "        \"num_data_workers\": 0,\n",
      "        \"hdf5_cache_mode\": \"all\",\n",
      "        \"hdf5_use_swmr\": true,\n",
      "        \"hdf5_normalize_obs\": false,\n",
      "        \"hdf5_filter_key\": null,\n",
      "        \"seq_length\": 10,\n",
      "        \"dataset_keys\": [\n",
      "            \"actions\",\n",
      "            \"rewards\",\n",
      "            \"dones\"\n",
      "        ],\n",
      "        \"goal_mode\": null,\n",
      "        \"cuda\": true,\n",
      "        \"batch_size\": 100,\n",
      "        \"num_epochs\": 2000,\n",
      "        \"seed\": 1\n",
      "    },\n",
      "    \"algo\": {\n",
      "        \"optim_params\": {\n",
      "            \"policy\": {\n",
      "                \"learning_rate\": {\n",
      "                    \"initial\": 0.0001,\n",
      "                    \"decay_factor\": 0.1,\n",
      "                    \"epoch_schedule\": []\n",
      "                },\n",
      "                \"regularization\": {\n",
      "                    \"L2\": 0.0\n",
      "                }\n",
      "            }\n",
      "        },\n",
      "        \"loss\": {\n",
      "            \"l2_weight\": 1.0,\n",
      "            \"l1_weight\": 0.0,\n",
      "            \"cos_weight\": 0.0\n",
      "        },\n",
      "        \"actor_layer_dims\": [],\n",
      "        \"gaussian\": {\n",
      "            \"enabled\": false,\n",
      "            \"fixed_std\": false,\n",
      "            \"init_std\": 0.1,\n",
      "            \"min_std\": 0.01,\n",
      "            \"std_activation\": \"softplus\",\n",
      "            \"low_noise_eval\": true\n",
      "        },\n",
      "        \"gmm\": {\n",
      "            \"enabled\": true,\n",
      "            \"num_modes\": 5,\n",
      "            \"min_std\": 0.0001,\n",
      "            \"std_activation\": \"softplus\",\n",
      "            \"low_noise_eval\": true\n",
      "        },\n",
      "        \"vae\": {\n",
      "            \"enabled\": false,\n",
      "            \"latent_dim\": 14,\n",
      "            \"latent_clip\": null,\n",
      "            \"kl_weight\": 1.0,\n",
      "            \"decoder\": {\n",
      "                \"is_conditioned\": true,\n",
      "                \"reconstruction_sum_across_elements\": false\n",
      "            },\n",
      "            \"prior\": {\n",
      "                \"learn\": false,\n",
      "                \"is_conditioned\": false,\n",
      "                \"use_gmm\": false,\n",
      "                \"gmm_num_modes\": 10,\n",
      "                \"gmm_learn_weights\": false,\n",
      "                \"use_categorical\": false,\n",
      "                \"categorical_dim\": 10,\n",
      "                \"categorical_gumbel_softmax_hard\": false,\n",
      "                \"categorical_init_temp\": 1.0,\n",
      "                \"categorical_temp_anneal_step\": 0.001,\n",
      "                \"categorical_min_temp\": 0.3\n",
      "            },\n",
      "            \"encoder_layer_dims\": [\n",
      "                300,\n",
      "                400\n",
      "            ],\n",
      "            \"decoder_layer_dims\": [\n",
      "                300,\n",
      "                400\n",
      "            ],\n",
      "            \"prior_layer_dims\": [\n",
      "                300,\n",
      "                400\n",
      "            ]\n",
      "        },\n",
      "        \"rnn\": {\n",
      "            \"enabled\": true,\n",
      "            \"horizon\": 10,\n",
      "            \"hidden_dim\": 400,\n",
      "            \"rnn_type\": \"LSTM\",\n",
      "            \"num_layers\": 2,\n",
      "            \"open_loop\": false,\n",
      "            \"kwargs\": {\n",
      "                \"bidirectional\": false\n",
      "            }\n",
      "        }\n",
      "    },\n",
      "    \"observation\": {\n",
      "        \"modalities\": {\n",
      "            \"obs\": {\n",
      "                \"low_dim\": [\n",
      "                    \"robot0_eef_pos\",\n",
      "                    \"robot0_eef_quat\",\n",
      "                    \"robot0_gripper_qpos\",\n",
      "                    \"object\"\n",
      "                ],\n",
      "                \"image\": []\n",
      "            },\n",
      "            \"goal\": {\n",
      "                \"low_dim\": [],\n",
      "                \"image\": []\n",
      "            }\n",
      "        },\n",
      "        \"encoder\": {\n",
      "            \"visual_core\": \"ResNet18Conv\",\n",
      "            \"visual_core_kwargs\": {\n",
      "                \"pretrained\": false,\n",
      "                \"input_coord_conv\": false\n",
      "            },\n",
      "            \"obs_randomizer_class\": null,\n",
      "            \"obs_randomizer_kwargs\": {\n",
      "                \"crop_height\": 76,\n",
      "                \"crop_width\": 76,\n",
      "                \"num_crops\": 1,\n",
      "                \"pos_enc\": false\n",
      "            },\n",
      "            \"visual_feature_dimension\": 64,\n",
      "            \"use_spatial_softmax\": true,\n",
      "            \"spatial_softmax_kwargs\": {\n",
      "                \"num_kp\": 32,\n",
      "                \"learnable_temperature\": false,\n",
      "                \"temperature\": 1.0,\n",
      "                \"noise_std\": 0.0\n",
      "            }\n",
      "        }\n",
      "    }\n",
      "}\n",
      "\n",
      "============= Initialized Observation Utils with Obs Spec =============\n",
      "\n",
      "using obs modality: low_dim with keys: ['robot0_eef_pos', 'robot0_eef_quat', 'robot0_gripper_qpos', 'object']\n",
      "using obs modality: image with keys: []\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m device \u001b[38;5;241m=\u001b[39m TorchUtils\u001b[38;5;241m.\u001b[39mget_torch_device(try_to_use_cuda\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# restore policy\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m policy, ckpt_dict \u001b[38;5;241m=\u001b[39m \u001b[43mFileUtils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy_from_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/rl/robomimic/robomimic/utils/file_utils.py:268\u001b[0m, in \u001b[0;36mpolicy_from_checkpoint\u001b[0;34m(device, ckpt_path, ckpt_dict, verbose)\u001b[0m\n\u001b[1;32m    265\u001b[0m config, _ \u001b[38;5;241m=\u001b[39m config_from_checkpoint(algo_name\u001b[38;5;241m=\u001b[39malgo_name, ckpt_dict\u001b[38;5;241m=\u001b[39mckpt_dict, verbose\u001b[38;5;241m=\u001b[39mverbose)\n\u001b[1;32m    267\u001b[0m \u001b[38;5;66;03m# read config to set up metadata for observation modalities (e.g. detecting rgb observations)\u001b[39;00m\n\u001b[0;32m--> 268\u001b[0m \u001b[43mObsUtils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitialize_obs_utils_with_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;66;03m# env meta from model dict to get info needed to create model\u001b[39;00m\n\u001b[1;32m    271\u001b[0m env_meta \u001b[38;5;241m=\u001b[39m ckpt_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menv_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/workspace/rl/robomimic/robomimic/utils/obs_utils.py:253\u001b[0m, in \u001b[0;36minitialize_obs_utils_with_config\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    251\u001b[0m     obs_encoder_config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mobservation\u001b[38;5;241m.\u001b[39mencoder\n\u001b[1;32m    252\u001b[0m initialize_obs_utils_with_obs_specs(obs_modality_specs\u001b[38;5;241m=\u001b[39mobs_modality_specs)\n\u001b[0;32m--> 253\u001b[0m \u001b[43minitialize_default_obs_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs_encoder_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobs_encoder_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/rl/robomimic/robomimic/utils/obs_utils.py:225\u001b[0m, in \u001b[0;36minitialize_default_obs_encoder\u001b[0;34m(obs_encoder_config)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;124;03mInitializes the default observation encoder kwarg information to be used by all networks if no values are manually\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;124;03mspecified at runtime.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;124;03m        Should be equivalent to config.observation.encoder\u001b[39;00m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m DEFAULT_ENCODER_KWARGS\n\u001b[0;32m--> 225\u001b[0m DEFAULT_ENCODER_KWARGS \u001b[38;5;241m=\u001b[39m \u001b[43mobs_encoder_kwargs_from_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs_encoder_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/rl/robomimic/robomimic/utils/obs_utils.py:99\u001b[0m, in \u001b[0;36mobs_encoder_kwargs_from_config\u001b[0;34m(obs_encoder_config)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m obs_modality, encoder_kwargs \u001b[38;5;129;01min\u001b[39;00m obs_encoder_config\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;66;03m# First run some sanity checks and store the classes\u001b[39;00m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m cls_name, cores \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcore\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobs_randomizer\u001b[39m\u001b[38;5;124m\"\u001b[39m), (OBS_ENCODER_CORES, OBS_RANDOMIZERS)):\n\u001b[1;32m     98\u001b[0m         \u001b[38;5;66;03m# Make sure the requested encoder for each obs_modality exists\u001b[39;00m\n\u001b[0;32m---> 99\u001b[0m         cfg_cls \u001b[38;5;241m=\u001b[39m \u001b[43mencoder_kwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcls_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_class\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    100\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m cfg_cls \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    101\u001b[0m             \u001b[38;5;28;01massert\u001b[39;00m cfg_cls \u001b[38;5;129;01min\u001b[39;00m cores, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcls_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m class with name \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcfg_cls\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m found, must register this class before\u001b[39m\u001b[38;5;124m\"\u001b[39m \\\n\u001b[1;32m    102\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreating model!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "device = TorchUtils.get_torch_device(try_to_use_cuda=True)\n",
    "\n",
    "# restore policy\n",
    "policy, ckpt_dict = FileUtils.policy_from_checkpoint(ckpt_path=ckpt_path, device=device, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2872a3f0",
   "metadata": {},
   "source": [
    "### Creating rollout envionment\n",
    "The policy checkpoint also contains sufficient information to recreate the environment that it's trained with. Again, you may manually create the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12d00c2a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ckpt_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# create environment from saved checkpoint\u001b[39;00m\n\u001b[1;32m      2\u001b[0m env, _ \u001b[38;5;241m=\u001b[39m FileUtils\u001b[38;5;241m.\u001b[39menv_from_checkpoint(\n\u001b[0;32m----> 3\u001b[0m     ckpt_dict\u001b[38;5;241m=\u001b[39m\u001b[43mckpt_dict\u001b[49m, \n\u001b[1;32m      4\u001b[0m     render\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;66;03m# we won't do on-screen rendering in the notebook\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     render_offscreen\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;66;03m# render to RGB images for video\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      7\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ckpt_dict' is not defined"
     ]
    }
   ],
   "source": [
    "# create environment from saved checkpoint\n",
    "env, _ = FileUtils.env_from_checkpoint(\n",
    "    ckpt_dict=ckpt_dict, \n",
    "    render=False, # we won't do on-screen rendering in the notebook\n",
    "    render_offscreen=True, # render to RGB images for video\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ac0e9f",
   "metadata": {},
   "source": [
    "### Define the rollout loop\n",
    "Now let's define the main rollout loop. The loop runs the policy to a target `horizon` and optionally writes the rollout to a video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd1375e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout(policy, env, horizon, render=False, video_writer=None, video_skip=5, camera_names=None):\n",
    "    \"\"\"\n",
    "    Helper function to carry out rollouts. Supports on-screen rendering, off-screen rendering to a video, \n",
    "    and returns the rollout trajectory.\n",
    "    Args:\n",
    "        policy (instance of RolloutPolicy): policy loaded from a checkpoint\n",
    "        env (instance of EnvBase): env loaded from a checkpoint or demonstration metadata\n",
    "        horizon (int): maximum horizon for the rollout\n",
    "        render (bool): whether to render rollout on-screen\n",
    "        video_writer (imageio writer): if provided, use to write rollout to video\n",
    "        video_skip (int): how often to write video frames\n",
    "        camera_names (list): determines which camera(s) are used for rendering. Pass more than\n",
    "            one to output a video with multiple camera views concatenated horizontally.\n",
    "    Returns:\n",
    "        stats (dict): some statistics for the rollout - such as return, horizon, and task success\n",
    "    \"\"\"\n",
    "    assert isinstance(env, EnvBase)\n",
    "    assert isinstance(policy, RolloutPolicy)\n",
    "    assert not (render and (video_writer is not None))\n",
    "\n",
    "    policy.start_episode()\n",
    "    obs = env.reset()\n",
    "    state_dict = env.get_state()\n",
    "\n",
    "    # hack that is necessary for robosuite tasks for deterministic action playback\n",
    "    obs = env.reset_to(state_dict)\n",
    "\n",
    "    results = {}\n",
    "    video_count = 0  # video frame counter\n",
    "    total_reward = 0.\n",
    "    try:\n",
    "        for step_i in range(horizon):\n",
    "\n",
    "            # get action from policy\n",
    "            act = policy(ob=obs)\n",
    "\n",
    "            # play action\n",
    "            next_obs, r, done, _ = env.step(act)\n",
    "\n",
    "            # compute reward\n",
    "            total_reward += r\n",
    "            success = env.is_success()[\"task\"]\n",
    "\n",
    "            # visualization\n",
    "            if render:\n",
    "                env.render(mode=\"human\", camera_name=camera_names[0])\n",
    "            if video_writer is not None:\n",
    "                if video_count % video_skip == 0:\n",
    "                    video_img = []\n",
    "                    for cam_name in camera_names:\n",
    "                        video_img.append(env.render(mode=\"rgb_array\", height=512, width=512, camera_name=cam_name))\n",
    "                    video_img = np.concatenate(video_img, axis=1) # concatenate horizontally\n",
    "                    video_writer.append_data(video_img)\n",
    "                video_count += 1\n",
    "\n",
    "            # break if done or if success\n",
    "            if done or success:\n",
    "                break\n",
    "\n",
    "            # update for next iter\n",
    "            obs = deepcopy(next_obs)\n",
    "            state_dict = env.get_state()\n",
    "\n",
    "    except env.rollout_exceptions as e:\n",
    "        print(\"WARNING: got rollout exception {}\".format(e))\n",
    "\n",
    "    stats = dict(Return=total_reward, Horizon=(step_i + 1), Success_Rate=float(success))\n",
    "\n",
    "    return stats, traj\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b43d371",
   "metadata": {},
   "source": [
    "### Run the policy\n",
    "Now let's rollout the policy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6e1878",
   "metadata": {},
   "outputs": [],
   "source": [
    "rollout_horizon = 400\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "video_path = \"rollout.mp4\"\n",
    "video_writer = imageio.get_writer(video_path, fps=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa67efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = rollout(\n",
    "    policy=policy, \n",
    "    env=env, \n",
    "    horizon=rollout_horizon, \n",
    "    render=args.render, \n",
    "    video_writer=video_writer, \n",
    "    video_skip=args.video_skip, \n",
    "    return_obs=(write_dataset and args.dataset_obs),\n",
    "    camera_names=args.camera_names,\n",
    ")\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe79bc19",
   "metadata": {},
   "source": [
    "### Visualize the rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97472b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "Video(video_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
